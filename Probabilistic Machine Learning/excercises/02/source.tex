\documentclass[UTF8]{article}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathrsfs}
\title{Probability: Univariate Models}
\date{}
\begin{document}
\maketitle

\noindent 1.

\noindent a.

\begin{equation}
    P(H, e_1, e_2) = P(e_1, e_2 | H)P(H)\label{eq:1-1}
\end{equation}
 

\begin{align}
    P(H | e_1, e_2) &= \frac{P(H, e_1, e_2)}{P(e_1, e_2)}\notag\\
                    &= \frac{P(e_1, e_2 | H)P(H)}{P(e_1, e_2)}\label{eq:1-2}
\end{align}

Therefore, the second sets of numbers are sufficient for the calculation.

\noindent b.

from $E_1\perp E_2 | H$, we know that:

\begin{equation}
    P(e_1, e_2| H) = P(e_1|H)P(e_2|H)\label{eq:1-3}
\end{equation}

Therefore, from equations \ref{eq:1-3} and \ref{eq:1-2}, there are:

\begin{align}
    P(H | e_1, e_2) &= \frac{P(e_1, e_2 | H)P(H)}{P(e_1, e_2)}\notag\\
                    &= \frac{P(e_1|H)P(e_2|H)P(H)}{P(e_1, e_2)}
\end{align}

Therefore, the first sets of numbers are sufficient for the calculation.

\noindent 2.

Suppose there are two boxes, both boxes have two balls colored separately in red
and yellow. We randomly choose one ball from each box. If we get two balls in the same color,
then we get mark 0. Otherewise, we get mark 1. And we define the following random variable:

$$X_1\in\{0, 1\}, $$

$X_1 = 0$, if the ball from box 1 is red; $X_1 = 1$if the ball from box 1 is yellow.

$$X_2\in\{0, 1\}, $$

$X_2 = 0$, if the ball from box 2 is red; $X_2 = 1$if the ball from box 1 is yellow.

$$X_3\in\{0, 1\}, $$

$X_3$ is the mark we get after choosing balls from these two boxes.

Therefore, there are

$$P(X_1 = 0) = 0.5, P(X_1 = 1) = 0.5$$
$$P(X_2 = 0) = 0.5, P(X_2 = 1) = 0.5$$
$$P(X_3 = 0) = 0.5, P(X_3 = 1) = 0.5$$

And obviously, there are

$$P(X_1 | X_2) = P(X_1), P(X_2 | X_1) = P(X_2)$$

And

\begin{align*}
    P(X_2 = 0 | X_3 = 0) &= \frac{P(X_2 = 0, X_3 = 0)}{P(X_3 = 0)}\\
    &= \frac{P(X_1 = 0, X_2 = 0, X_3 = 0)+P(X_1 = 1, X_2=0, X_3 = 0)}{P(X_3 = 0)}\\
    &= \frac{0.25 + 0}{0.5} = 0.5 = P(X_2 = 0)
\end{align*}

Similarily, there are
$$P(X_2 = 1 | X_3 = 0) =P(X_2 = 1),  P(X_2 = 0 | X_3 = 1) =P(X_2 = 0), P(X_2 = 1 | X_3 = 1) =P(X_2 = 1)$$

Therefore,
$$P(X_2 | X_3) = P(X_2), P(X_3 | X_2) = P(X_3)$$

Similarily, we can get
$$P(X_1 | X_3) = P(X_1), P(X_3 | X_1) = P(X_1)$$

Therefore, $X_1, X_2, X_3$ are pairwise independent.

However,

$$P(X_1 = 0, X_2 = 0, X_3 = 1) = 0 \neq P(X_1 = 0)P(X_2 = 0)P(X_3 = 1)$$

Therefore, pairwise independence between all pairs of variables does not necessarily imply mutual independence.

\noindent 3.

It is obvious that if

$$p(x,y|z)=p(x|z)p(y|z)$$

then we let $g(x,z)=p(x|z), h(y,z)=p(y|z)$. Therefore, 

$$p(x,y|z)=g(x,z)h(y,z)$$

And if there is

$$p(x,y|z)=g(x,z)h(y,z)$$

then,

\begin{align*}
    p(y|z) = \int p(x,y|z)dx = h(y,z)\int g(x,z)dx
\end{align*}

We mark $q(z) = \int g(x,z)dx$, Therefore,

$$p(y|z) = h(y,z)q(z)$$

Similarily, we can get

$$p(x|z) = g(x,z)r(z)$$

Where $r(z) = \int h(y,z)dy$

Therefore

\begin{align*}
    p(x,y|z) &= \frac{p(x|z)p(y|z)}{r(z)q(z)} \\
    \int p(x,y|z)dx& = \frac{p(y|z)}{r(z)q(z)}\int p(x|z)dx\\
    p(y|z) &= \frac{p(y|z)}{r(z)q(z)}\\
    r(z)q(z) & = 1
\end{align*}

Therefore, there is:

$$p(x,y|z) = p(x|z)p(y|z)$$

\qedsymbol

\noindent 3.

With the definition of convolution, we have:

\begin{align*}
    p(y)& = \int_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi\sigma_1^2}}e^{-\frac{1}{2\sigma_1^2}(\tau-\mu_1)^2}\frac{1}{\sqrt{2\pi\sigma_2^2}}e^{-\frac{1}{2\sigma_2^2}(y-\tau-\mu_2)^2}d\tau\\
        &= \int_{-\infty}^{+\infty}\frac{1}{2\pi\sigma_1\sigma_2}e^{-\frac{1}{2\sigma_1^2\sigma_2^2}[(\sigma_1^2+\sigma_2^2)\tau^2+(2\sigma_1^2-2\sigma_2^2\mu_1-2\sigma_1^2y)\tau+\sigma_2^2\mu_1^2+\sigma_1^2y^2-2\sigma_1^2\mu_2y+\sigma_1^2\mu_2^2]}d\tau\\
        &= \int_{-\infty}^{+\infty}\frac{1}{2\pi\sigma_1\sigma_2}e^{-\frac{\sigma_1^2+\sigma_2^2}{2\sigma_1^2\sigma_2^2}[\tau^2-2\frac{\sigma_1^2 y+\sigma_2^2 \mu_1-\sigma_1^2 \mu_2}{\sigma_1^2+\sigma_2^2}\tau+(\frac{\sigma_1^2 y+\sigma_2^2 \mu_1-\sigma_1^2 \mu_2}{\sigma_1^2+\sigma_2^2})^2-(\frac{\sigma_1^2 y+\sigma_2^2 \mu_1-\sigma_1^2 \mu_2}{\sigma_1^2+\sigma_2^2})^2+\frac{\sigma_2^2 \mu_1^2+\sigma_1^2(y-\mu_2)^2}{\sigma_1^2+\sigma_2^2}]}d\tau\\
        &= \int_{-\infty}^{+\infty}\frac{1}{2\pi\sigma_1\sigma_2}e^{-\frac{\sigma_1^2+\sigma_2^2}{2\sigma_1^2\sigma_2^2}[(\tau-\frac{\sigma_1^2 y+\sigma_2^2 \mu_1-\sigma_1^2 \mu_2}{\sigma_1^2+\sigma_2^2})^2+\frac{\sigma_1^2\sigma_2^2(y-\mu_1-\mu_2)^2}{(\sigma_1^2+\sigma_2^2)^2}]}d\tau\\
        & = \frac{1}{\sqrt{2\pi(\sigma_1^2+\sigma_2^2)}}e^{-\frac{(y-\mu_1-\mu_2)^2}{2(\sigma_1^2+\sigma_2^2)}}\int_{-\infty}^{+\infty}\frac{1}{2\pi\sqrt{\frac{\sigma_1^2\sigma_2^2}{\sigma_1^2+\sigma_2^2}}}e^{-\frac{\sigma_1^2+\sigma_2^2}{2\sigma_1^2\sigma_2^2}(\tau-\frac{\sigma_1^2 y+\sigma_2^2 \mu_1-\sigma_1^2 \mu_2}{\sigma_1^2+\sigma_2^2})^2}d\tau\\
        & = \frac{1}{\sqrt{2\pi(\sigma_1^2+\sigma_2^2)}}e^{-\frac{(y-\mu_1-\mu_2)^2}{2(\sigma_1^2+\sigma_2^2)}}
\end{align*}

Therefore, $p(y) = \mathcal{N}(y|\mu_1+\mu_2, \sigma_2^2+\sigma_2^2)$

\end{document}