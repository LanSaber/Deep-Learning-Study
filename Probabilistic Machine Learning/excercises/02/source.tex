\documentclass[UTF8]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathrsfs}
\title{Probability: Univariate Models}
\date{}
\begin{document}
\maketitle

\noindent 1.

\noindent a.

\begin{equation}
    P(H, e_1, e_2) = P(e_1, e_2 | H)P(H)\label{eq:1-1}
\end{equation}
 

\begin{align}
    P(H | e_1, e_2) &= \frac{P(H, e_1, e_2)}{P(e_1, e_2)}\notag\\
                    &= \frac{P(e_1, e_2 | H)P(H)}{P(e_1, e_2)}\label{eq:1-2}
\end{align}

Therefore, the second sets of numbers are sufficient for the calculation.

\noindent b.

from $E_1\perp E_2 | H$, we know that:

\begin{equation}
    P(e_1, e_2| H) = P(e_1|H)P(e_2|H)\label{eq:1-3}
\end{equation}

Therefore, from equations \ref{eq:1-3} and \ref{eq:1-2}, there are:

\begin{align}
    P(H | e_1, e_2) &= \frac{P(e_1, e_2 | H)P(H)}{P(e_1, e_2)}\notag\\
                    &= \frac{P(e_1|H)P(e_2|H)P(H)}{P(e_1, e_2)}
\end{align}

Therefore, the first sets of numbers are sufficient for the calculation.

\noindent 2.

Suppose there are two boxes, both boxes have two balls colored separately in red
and yellow. We randomly choose one ball from each box. If we get two balls in the same color,
then we get mark 0. Otherewise, we get mark 1. And we define the following random variable:

$$X_1\in\{0, 1\}, $$

$X_1 = 0$, if the ball from box 1 is red; $X_1 = 1$if the ball from box 1 is yellow.

$$X_2\in\{0, 1\}, $$

$X_2 = 0$, if the ball from box 2 is red; $X_2 = 1$if the ball from box 1 is yellow.

$$X_3\in\{0, 1\}, $$

$X_3$ is the mark we get after choosing balls from these two boxes.

Therefore, there are

$$P(X_1 = 0) = 0.5, P(X_1 = 1) = 0.5$$
$$P(X_2 = 0) = 0.5, P(X_2 = 1) = 0.5$$
$$P(X_3 = 0) = 0.5, P(X_3 = 1) = 0.5$$

And obviously, there are

$$P(X_1 | X_2) = P(X_1), P(X_2 | X_1) = P(X_2)$$

And

\begin{align*}
    P(X_2 = 0 | X_3 = 0) &= \frac{P(X_2 = 0, X_3 = 0)}{P(X_3 = 0)}\\
    &= \frac{P(X_1 = 0, X_2 = 0, X_3 = 0)+P(X_1 = 1, X_2=0, X_3 = 0)}{P(X_3 = 0)}\\
    &= \frac{0.25 + 0}{0.5} = 0.5 = P(X_2 = 0)
\end{align*}

Similarily, there are
$$P(X_2 = 1 | X_3 = 0) =P(X_2 = 1),  P(X_2 = 0 | X_3 = 1) =P(X_2 = 0), P(X_2 = 1 | X_3 = 1) =P(X_2 = 1)$$

Therefore,
$$P(X_2 | X_3) = P(X_2), P(X_3 | X_2) = P(X_3)$$

Similarily, we can get
$$P(X_1 | X_3) = P(X_1), P(X_3 | X_1) = P(X_1)$$

Therefore, $X_1, X_2, X_3$ are pairwise independent.

However,

$$P(X_1 = 0, X_2 = 0, X_3 = 1) = 0 \neq P(X_1 = 0)P(X_2 = 0)P(X_3 = 1)$$

Therefore, pairwise independence between all pairs of variables does not necessarily imply mutual independence.

\noindent 3.

It is obvious that if

$$p(x,y|z)=p(x|z)p(y|z)$$

then we let $g(x,z)=p(x|z), h(y,z)=p(y|z)$. Therefore, 

$$p(x,y|z)=g(x,z)h(y,z)$$

And if there is

$$p(x,y|z)=g(x,z)h(y,z)$$

then,

\begin{align*}
    p(y|z) = \int p(x,y|z)dx = h(y,z)\int g(x,z)dx
\end{align*}

We mark $q(z) = \int g(x,z)dx$, Therefore,

$$p(y|z) = h(y,z)q(z)$$

Similarily, we can get

$$p(x|z) = g(x,z)r(z)$$

Where $r(z) = \int h(y,z)dy$

Therefore

\begin{align*}
    p(x,y|z) &= \frac{p(x|z)p(y|z)}{r(z)q(z)} \\
    \int p(x,y|z)dx& = \frac{p(y|z)}{r(z)q(z)}\int p(x|z)dx\\
    p(y|z) &= \frac{p(y|z)}{r(z)q(z)}\\
    r(z)q(z) & = 1
\end{align*}

Therefore, there is:

$$p(x,y|z) = p(x|z)p(y|z)$$

\qedsymbol

\noindent 4.

With the definition of convolution, we have:

\begin{align*}
    p(y)& = \int_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi\sigma_1^2}}e^{-\frac{1}{2\sigma_1^2}(\tau-\mu_1)^2}\frac{1}{\sqrt{2\pi\sigma_2^2}}e^{-\frac{1}{2\sigma_2^2}(y-\tau-\mu_2)^2}d\tau\\
        &= \int_{-\infty}^{+\infty}\frac{1}{2\pi\sigma_1\sigma_2}e^{-\frac{1}{2\sigma_1^2\sigma_2^2}[(\sigma_1^2+\sigma_2^2)\tau^2+(2\sigma_1^2-2\sigma_2^2\mu_1-2\sigma_1^2y)\tau+\sigma_2^2\mu_1^2+\sigma_1^2y^2-2\sigma_1^2\mu_2y+\sigma_1^2\mu_2^2]}d\tau\\
        &= \int_{-\infty}^{+\infty}\frac{1}{2\pi\sigma_1\sigma_2}e^{-\frac{\sigma_1^2+\sigma_2^2}{2\sigma_1^2\sigma_2^2}[\tau^2-2\frac{\sigma_1^2 y+\sigma_2^2 \mu_1-\sigma_1^2 \mu_2}{\sigma_1^2+\sigma_2^2}\tau+(\frac{\sigma_1^2 y+\sigma_2^2 \mu_1-\sigma_1^2 \mu_2}{\sigma_1^2+\sigma_2^2})^2-(\frac{\sigma_1^2 y+\sigma_2^2 \mu_1-\sigma_1^2 \mu_2}{\sigma_1^2+\sigma_2^2})^2+\frac{\sigma_2^2 \mu_1^2+\sigma_1^2(y-\mu_2)^2}{\sigma_1^2+\sigma_2^2}]}d\tau\\
        &= \int_{-\infty}^{+\infty}\frac{1}{2\pi\sigma_1\sigma_2}e^{-\frac{\sigma_1^2+\sigma_2^2}{2\sigma_1^2\sigma_2^2}[(\tau-\frac{\sigma_1^2 y+\sigma_2^2 \mu_1-\sigma_1^2 \mu_2}{\sigma_1^2+\sigma_2^2})^2+\frac{\sigma_1^2\sigma_2^2(y-\mu_1-\mu_2)^2}{(\sigma_1^2+\sigma_2^2)^2}]}d\tau\\
        & = \frac{1}{\sqrt{2\pi(\sigma_1^2+\sigma_2^2)}}e^{-\frac{(y-\mu_1-\mu_2)^2}{2(\sigma_1^2+\sigma_2^2)}}\int_{-\infty}^{+\infty}\frac{1}{2\pi\sqrt{\frac{\sigma_1^2\sigma_2^2}{\sigma_1^2+\sigma_2^2}}}e^{-\frac{\sigma_1^2+\sigma_2^2}{2\sigma_1^2\sigma_2^2}(\tau-\frac{\sigma_1^2 y+\sigma_2^2 \mu_1-\sigma_1^2 \mu_2}{\sigma_1^2+\sigma_2^2})^2}d\tau\\
        & = \frac{1}{\sqrt{2\pi(\sigma_1^2+\sigma_2^2)}}e^{-\frac{(y-\mu_1-\mu_2)^2}{2(\sigma_1^2+\sigma_2^2)}}
\end{align*}

Therefore, $p(y) = \mathcal{N}(y|\mu_1+\mu_2, \sigma_2^2+\sigma_2^2)$

\noindent 5.

From the information we know, there is:

\begin{align*}
    P(\min(x,y)\leq t) &= P(x\leq t , y \leq t) + P(x\leq t , y \ge t) + P(x\ge t , y \leq t) \\
    &=t^2 + 2t(1-t)\\
    &=2t - t^2
\end{align*}

Therefore,

\begin{align*}
    p(\min(x,y)= t) &= \frac{dP}{dt}\\
    &=2 - 2t
\end{align*}

Therefore, we can get,

\begin{align*}
    \mathbb{E}(\min(x,y)= t) &= \int_{0}^{1}2t-2t^2dt\\
    &=(t^2 - \frac{2t^3}{3})|_{0}^{1}\\
    &=\frac{1}{3}
\end{align*}

Therefore, the expected location of the ;eftmost point is $\frac{1}{3}$

\noindent 6.

\begin{align*}
    \mathbb{V}[X+Y] &= \mathbb{E}[(X+Y)^2] - \mathbb{E}^2(X+Y)\\
                    &= \mathbb{E}(X^2) + \mathbb{E}(Y^2) + 2\mathbb{E}(XY) - (\mathbb{E}^2(X)+2\mathbb{E}(X)\mathbb{E}(Y)+\mathbb{E}^2(Y))\\
                    &= \mathbb{E}(X^2) - \mathbb{E}^2(X) + \mathbb{E}(Y^2) - \mathbb{E}^2(Y) + 2(\mathbb{E}(XY) - \mathbb{E}(X)\mathbb{E}(Y))\\
                    &= \mathbb{V}[X] + \mathbb{V}[Y] + 2\text{Cov}[X, Y]
\end{align*}

\noindent 7.

\begin{align*}
    p(x|a, b) = \frac{b^a}{\Gamma(a)}x^{a-1}e^{-xb}
\end{align*}

Because $Y=1/X$, therefore,

\begin{align*}
    p(y|a, b) = \frac{b^a}{\Gamma(a)}y^{1-a}e^{-\frac{b}{y}}
\end{align*}

\noindent 8.

\begin{align*}
    p(\theta|a, b) &= \frac{1}{B(a, b)}\theta^{a-1}(1-\theta)^{b-1}\\
    \frac{dp}{dt}&= \frac{1}{B(a, b)}[(a-1)\theta^{a-2}(1-\theta)^{b-1}-(b-1)\theta^{a-1}(1-\theta)^{b-2}]
\end{align*}

Therefore, if $\frac{dp}{dt} = 0$, then, 

\begin{align*}
    (a-1)\theta^{a-2}(1-\theta)^{b-1}&=(b-1)\theta^{a-1}(1-\theta)^{b-2}\\
    (a-1)(1-\theta) &= (b-1)\theta\\
    \theta &= \frac{a-1}{a+b-2}
\end{align*}

Therefore, $\text{mode}(\theta) = \frac{a-1}{a+b-2}$

\begin{align*}
    \mathbb{E}[\theta] = \frac{1}{B(a, b)}\int_{0}^{1}\theta^{a}(1-\theta)^{b-1}d\theta
\end{align*}

We know that

\begin{align*}
    \int_{0}^{1}p(\theta|a+1, b) = \frac{1}{B(a+1, b)}\int_{0}^{1}\theta^{a}(1-\theta)^{b-1}d\theta = 1
\end{align*}

Therefore

\begin{align*}
    \int_{0}^{1}\theta^{a}(1-\theta)^{b-1}d\theta = B(a+1, b)
\end{align*}

So

\begin{align*}
    \mathbb{E}[\theta] = \frac{B(a+1, b)}{B(a, b)} = \frac{a}{a+b}
\end{align*}

\begin{align*}
    \mathbb{E}[\theta^2] &= \frac{1}{B(a, b)}\int_{0}^{1}\theta^{a+1}(1-\theta)^{b-1}d\theta\\
    &=\frac{B(a+2, b)}{B(a, b)}\\
    &=\frac{a(a+1)}{(a+b)(a+b+1)}\\
\end{align*}

Therefore,

\begin{align*}
    \mathbb{V}[\theta] &= \mathbb{E}[\theta^2] - \mathbb{E}^2[\theta]\\
    &=\frac{ab}{(a+b)^2(a+b+1)}\\
\end{align*}

\noindent 9.

From the information, we know that
\begin{equation*}
\begin{aligned}
    P(T=1|D=1) &= 0.99&P(T=0|D=0)&= 0.99\\
    P(T=0|D=1) &= 0.01&P(T=1|D=0)&= 0.01\\
    P(D=1) &= 10^{-4}&P(D=0) &= 1-10^{-4}\\
\end{aligned}
\end{equation*}

Therefore, 

\begin{align*}
    P(D=1|T=1) &= \frac{P(T=1,D=1)}{P(T=1)}\\
               &= \frac{P(T=1|D=1)P(D=1)}{P(T=1|D=0)P(D=0)+P(T=1|D=1)P(D=1)}\\
               &= \frac{0.99*0.0001}{0.01*(1-0.0001)+0.99*0.0001}\\
               &= \frac{1}{102}
\end{align*}

So there is only a $\frac{1}{102}$ probability having the disease.

\noindent 10.

We set the random variable $C$ represents whether the person did the crime. $C=1$ means he did. $C=0$ mans he did not. And
We set the random variable $B$ represents whether the person's blood type is the same as the evidence's. $B=1$ means it is. $B=0$ mans it is not. From
the information we know that$P(B=1)=0.01$ and $P(B=1|C=1)=1$

\noindent a.

The prosecutor means if $P(B=1|C=0)$ is $0.01$, then $P(C=1|B=1)$ is 0.99, which is wrong. Because it is $P(B=0|C=0)$ is 0.99 instead of
$P(C=1|B=1)$.

\noindent b.

From the information we know that $P(C=1) = \frac{1}{800000}$, Therefore $P(C=1|B=1) = \frac{P(C=1, B=1)}{P(B=1)}=\frac{P(B=1|C=1)P(C=1)}{P(B=1)} = \frac{\frac{1}{800000}}{0.01}=\frac{1}{8000}$, so there is
definitely relevant.

\noindent 11.

\noindent a.

\begin{align*}
    P(G=1|B=1) &=  \frac{P(G=1, B=1)}{P(B=1)}\\
    &=\frac{\frac{1}{2}}{\frac{3}{4}}\\
    &=\frac{2}{3}
\end{align*}

\noindent b.

The gender of the other child is independent from this child, so the probability is $\frac{1}{2}$.

\noindent 12.

\begin{align*}
    \int_{0}^{2\pi}\int_{0}^{+\infty}re^{-\frac{r^2}{2\sigma^2}}drd\theta&=\int_{0}^{2\pi}(-\sigma^2e^{-\frac{r^2}{2\sigma^2}})|_{0}^{+\infty}d\theta\\
    &=\int_{0}^{2\pi}\sigma^2d\theta\\
    &=2\pi\sigma^2
\end{align*}

Therefore, $Z=\sqrt{2\pi\sigma^2}$

\end{document}